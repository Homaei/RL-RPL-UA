# üì° RL-RPL-UA: Reinforcement Learning-Based Telematic Routing for the Internet of Underwater Things

This repository accompanies the paper titled:

**"A Reinforcement Learning-Based Telematic Routing Protocol for the Internet of Underwater Things"**  
üìù Presented at the XVII Jornadas de Ingenier√≠a Telem√°tica (JITEL 2025), C√°ceres, Spain

## üìÑ Abstract

The Internet of Underwater Things (IoUT) faces challenges such as high latency, limited bandwidth, and energy constraints. Traditional routing protocols like RPL fall short in such environments. This work introduces **RL-RPL-UA**, an enhanced RPL-based routing protocol that integrates a lightweight reinforcement learning (RL) agent within each node to adaptively select the best parent based on local metrics like energy, link quality, buffer occupancy, and delivery history. Simulation results demonstrate that RL-RPL-UA significantly improves delivery ratio, energy efficiency, and network lifetime in both static and mobile underwater scenarios.

## üîç Highlights

- ‚úÖ Lightweight Q-learning agent embedded in each node  
- üì∂ Dynamic parent selection based on local observations  
- üß† Adaptive Objective Function (OF) replaces static RPL metrics  
- üí° Compatible with RPL control messages (DIO/DAO)  
- üìä Outperforms UWF-RPL, Co-DRAR, UA-RPL, and UWMRPL in Aqua-Sim simulations


## üîç Highlights of background

## üìä <strong>Table 1 ‚Äì Comparison of Recent Routing Protocols with RL-RPL-UA</strong>
  
| Protocol                   | RL   | RPL   | Adaptive OF        | Mobility | Citation        |
|----------------------------|------|-------|--------------------|----------|-----------------|
| C-GCo-DRAR                 | ‚ùå   | ‚ùå   | Static OF          | Limited  | Guo2023         |
| FLCEER                     | ‚ùå   | ‚ùå   | Static OF          | Moderate | Natesan2020     |
| IDA-OEP                    | ‚ùå   | ‚ùå   | Static OF          | Limited  | Wang2024        |
| GTRP                       | ‚ùå   | ‚ùå   | Static OF          | Moderate | Khan2021        |
| RL Protocol                | ‚úÖ   | ‚ùå   | Static OF          | Moderate | Eris2024        |
| Q-Learning                 | ‚úÖ   | ‚ùå   | Dynamic OF         | Moderate | Nandyala2023    |
| Multi-agent RL             | ‚úÖ   | ‚ùå   | Static OF          | High     | Li2020          |
| UA-RPL                     | ‚ùå   | ‚úÖ   | Static OF          | Moderate | Liu2022         |
| URPL                       | ‚ùå   | ‚úÖ   | Dynamic OF         | Moderate | Tarifdm2024     |
| Fuzzy-CR                   | ‚ùå   | ‚úÖ   | Decision Making    | Moderate | Tariffuzzy2024  |
| UWF-RPL                    | ‚ùå   | ‚úÖ   | Static Fuzzy OF    | Moderate | Tarif2025       |
| UW/MRPL (prev. work)       | ‚ùå   | ‚úÖ   | Static OF          | High     | Homaei2025      |
| **RL-RPL-UA (this work)**  | ‚úÖ   | ‚úÖ   | **Dynamic**        | High     | ‚Äî               |


## üìä <strong> Table 2 ‚Äì Key Differences Between UWF-RPL, UWMRPL, and RL-RPL-U  </strong>
| Feature               | UWF-RPL (Tarif2025)                              | UWMRPL (Homaei2025)                               | RL-RPL-UA (This Work)                                      |
|----------------------|--------------------------------------------------|---------------------------------------------------|-------------------------------------------------------------|
| Main Concept          | Fuzzy-logic RPL for optimized routing           | Mobility-aware RPL with static tunable OF         | RL-based dynamic routing with local agents                  |
| Routing Adaptability | Semi-adaptive via static fuzzy logic rules      | Semi-adaptive via predefined logic                | Fully adaptive via real-time RL updates                     |
| OF                   | Static fuzzy logic (depth, energy, latency, ETX)| Static/custom (ETX, depth)                        | Dynamic composite (energy, LQI, queue, PDR)                 |
| RPL Compatibility    | Extended DIO/DAO with fuzzy logic metrics       | Standard-compliant                                | Extended DIO/DAO with RL metrics                            |
| Learning Agent       | None (static fuzzy logic)                       | None                                              | Q-learning or DQN per node                                  |
| Reward Mechanism     | None                                            | None                                              | Œ±¬∑PDR ‚àí Œ≤¬∑Delay ‚àí Œ≥¬∑Cost                                    |
| Overhead             | Moderate (fuzzy logic computations)             | Low (no learning updates)                         | Low (optimized RL updates)                                  |
| Mobility Handling    | Reactive via fuzzy logic evaluation             | Reactive DAG repair                               | Proactive via learned feedback                              |
| Queue Management     | Included (congestion control)                   | Not included                                      | Included (adaptive queue management)                        |
| Energy Efficiency    | Good (static optimized)                         | Moderate (no dynamic optimization)                | High (real-time optimization)                               |
| Key Contribution     | Improved stability and efficiency via fuzzy logic | Mobility and energy-aware extension of RPL       | Online adaptive parent selection via RL                     |


## üìä <strong>Table 3 ‚Äì Simulation Parameters

| Parameter                 | Value                          |
|---------------------------|--------------------------------|
| Simulation Area           | 300 √ó 300 √ó 300 m¬≥             |
| Number of Nodes           | 50                             |
| Initial Energy per Node   | 5 J                            |
| Transmission Power        | 0.5 W                          |
| Acoustic Bandwidth        | 10 kHz                         |
| Propagation Speed         | 1500 m/s                       |
| MAC Protocol              | TDMA                           |
| Routing Protocols         | RL-RPL-UA, RPL (OF0), Q-Routing|
| RL Algorithm              | Q-learning (tabular)           |
| Learning Rate (Œ∑)         | 0.1                            |
| Discount Factor (Œ≥)       | 0.9                            |
| Reward Weights (Œ±, Œ≤, Œ≥)  | (1.0, 0.6, 0.4)                |
| Simulation Time           | 1000 s                         |
| Traffic Model             | CBR, 1 packet/10 s             |
| Packet Size               | 64 Bytes                       |
| Mobility Model            | Random Waypoint (0.1‚Äì0.3 m/s)  |


## üìä Simulation Results (Aqua-Sim / NS-2)

| Metric | RL-RPL-UA vs Baselines |
|--------|-------------------------|
| Packet Delivery Ratio | +9.2% over UWRPL |
| Energy per Packet | ‚àí14.8% energy use |
| Network Lifetime | +80 seconds |
| Routing Overhead | 45% lower |
| End-to-End Delay | 10‚Äì25% reduction |

## üß† Protocol Architecture

Each node includes:
- Sensing Unit
- RL Agent (Q-learning)
- Extended RPL Stack
- Acoustic MAC (TDMA)

Decision process modeled as an MDP with custom reward:

![Reward function](https://latex.codecogs.com/png.image?\dpi{110}&space;r_t=\alpha\cdot\text{PDR}_t-\beta\cdot\text{Delay}_t-\gamma\cdot\text{EnergyCost}_t)

---


## üìÅ Repository Contents

- `RL-RPL-UA.tex` ‚Äì Full LaTeX source of the manuscript
- `figures/` ‚Äì Static and mobile simulation graphs (PDR, delay, energy, lifetime)
- `aqua-sim-scenarios/` ‚Äì NS-2 / Aqua-Sim configuration files
- `readme/` ‚Äì Abstract, key tables, and citation formats

## üìö How to Cite This Work

### üìå APA (7th Edition)

Homaei, M., Tarif, M., Di Bartolo, A., & √Åvila Vegas, M. (2025). *A Reinforcement Learning-Based Telematic Routing Protocol for the Internet of Underwater Things*. JITEL 2025.

---

### üìå IEEE

M. Homaei, M. Tarif, A. Di Bartolo, and M. √Åvila Vegas, ‚ÄúA Reinforcement Learning-Based Telematic Routing Protocol for the Internet of Underwater Things,‚Äù in *Proc. XVII Jornadas de Ingenier√≠a Telem√°tica (JITEL 2025)*, C√°ceres, Spain, Nov. 2025.

---

### üìå BibTeX
```bibtex
@misc{homaei2025rlrplu,
  doi = {10.48550/ARXIV.2506.00133},
  url = {https://arxiv.org/abs/2506.00133},
  author = {Homaei,  Mohammadhossein and Tarif,  Mehran and Di Bartolo,  Agustin and Gutierrez, and Avila,  Mar},
  keywords = {Networking and Internet Architecture (cs.NI),  Artificial Intelligence (cs.AI),  Machine Learning (cs.LG),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {A Reinforcement Learning-Based Telematic Routing Protocol for the Internet of Underwater Things},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}
```
---
```bibtex
@inproceedings{homaei2025rlrplua,
  author    = {Homaei, Mohammadhossein and Tarif, Mehran and Di Bartolo, Agustin and √Åvila Vegas, Mar},
  title     = {A Reinforcement Learning-Based Telematic Routing Protocol for the Internet of Underwater Things},
  booktitle = {Proc. XVII Jornadas de Ingenier√≠a Telem√°tica (JITEL)},
  year      = {2025},
  address   = {C√°ceres, Spain},
  month     = {November}
}

